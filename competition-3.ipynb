{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":74732,"databundleVersionId":8159156,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-20T19:46:59.275416Z","iopub.execute_input":"2024-04-20T19:46:59.275884Z","iopub.status.idle":"2024-04-20T19:47:00.199970Z","shell.execute_reply.started":"2024-04-20T19:46:59.275841Z","shell.execute_reply":"2024-04-20T19:47:00.199009Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/cisc-873-dm-w24-a3/sample_submission.csv\n/kaggle/input/cisc-873-dm-w24-a3/x_test.csv\n/kaggle/input/cisc-873-dm-w24-a3/xy_train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport nltk\nimport re\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split,GridSearchCV, PredefinedSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(language='english')\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:47:04.965894Z","iopub.execute_input":"2024-04-20T19:47:04.966371Z","iopub.status.idle":"2024-04-20T19:47:06.902266Z","shell.execute_reply.started":"2024-04-20T19:47:04.966342Z","shell.execute_reply":"2024-04-20T19:47:06.901483Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Read the data**","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/cisc-873-dm-w24-a3/xy_train.csv')\ndf2=pd.read_csv('/kaggle/input/cisc-873-dm-w24-a3/x_test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:47:11.770373Z","iopub.execute_input":"2024-04-20T19:47:11.770762Z","iopub.status.idle":"2024-04-20T19:47:12.290654Z","shell.execute_reply.started":"2024-04-20T19:47:11.770734Z","shell.execute_reply":"2024-04-20T19:47:12.289605Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-20T08:23:32.869571Z","iopub.execute_input":"2024-04-20T08:23:32.870131Z","iopub.status.idle":"2024-04-20T08:23:32.902149Z","shell.execute_reply.started":"2024-04-20T08:23:32.870089Z","shell.execute_reply":"2024-04-20T08:23:32.900611Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           id                                               text  label\n0      265723  A group of friends began to volunteer at a hom...      0\n1      284269  British Prime Minister @Theresa_May on Nerve A...      0\n2      207715  In 1961, Goodyear released a kit that allows P...      0\n3      551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n4        8584  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0\n...       ...                                                ...    ...\n59995   70046  Finish Sniper Simo H盲yh盲 during the invasion o...      0\n59996  189377  Nigerian Prince Scam took $110K from Kansas ma...      1\n59997   93486  Is It Safe To Smoke Marijuana During Pregnancy...      0\n59998  140950  Julius Caesar upon realizing that everyone in ...      0\n59999   34509  Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...      1\n\n[60000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>265723</td>\n      <td>A group of friends began to volunteer at a hom...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>284269</td>\n      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>207715</td>\n      <td>In 1961, Goodyear released a kit that allows P...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>551106</td>\n      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8584</td>\n      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>70046</td>\n      <td>Finish Sniper Simo H盲yh盲 during the invasion o...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>189377</td>\n      <td>Nigerian Prince Scam took $110K from Kansas ma...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>93486</td>\n      <td>Is It Safe To Smoke Marijuana During Pregnancy...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>140950</td>\n      <td>Julius Caesar upon realizing that everyone in ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>34509</td>\n      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>60000 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df2","metadata":{"execution":{"iopub.status.busy":"2024-04-20T08:24:13.855107Z","iopub.execute_input":"2024-04-20T08:24:13.856182Z","iopub.status.idle":"2024-04-20T08:24:13.872614Z","shell.execute_reply.started":"2024-04-20T08:24:13.856132Z","shell.execute_reply":"2024-04-20T08:24:13.871101Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"          id                                               text\n0          0                                         stargazer \n1          1                                               yeah\n2          2  PD: Phoenix car thief gets instructions from Y...\n3          3  As Trump Accuses Iran, He Has One Problem: His...\n4          4                       \"Believers\" - Hezbollah 2011\n...      ...                                                ...\n59146  59146                  Bicycle taxi drivers of New Delhi\n59147  59147  Trump blows up GOP's formula for winning House...\n59148  59148  Napoleon returns from his exile on the island ...\n59149  59149   Deep down he always wanted to be a ballet dancer\n59150  59150  Toddler miraculously survives 6-story fall lan...\n\n[59151 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>stargazer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>PD: Phoenix car thief gets instructions from Y...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>As Trump Accuses Iran, He Has One Problem: His...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>\"Believers\" - Hezbollah 2011</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59146</th>\n      <td>59146</td>\n      <td>Bicycle taxi drivers of New Delhi</td>\n    </tr>\n    <tr>\n      <th>59147</th>\n      <td>59147</td>\n      <td>Trump blows up GOP's formula for winning House...</td>\n    </tr>\n    <tr>\n      <th>59148</th>\n      <td>59148</td>\n      <td>Napoleon returns from his exile on the island ...</td>\n    </tr>\n    <tr>\n      <th>59149</th>\n      <td>59149</td>\n      <td>Deep down he always wanted to be a ballet dancer</td>\n    </tr>\n    <tr>\n      <th>59150</th>\n      <td>59150</td>\n      <td>Toddler miraculously survives 6-story fall lan...</td>\n    </tr>\n  </tbody>\n</table>\n<p>59151 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T08:25:36.964871Z","iopub.execute_input":"2024-04-20T08:25:36.965360Z","iopub.status.idle":"2024-04-20T08:25:36.990953Z","shell.execute_reply.started":"2024-04-20T08:25:36.965318Z","shell.execute_reply":"2024-04-20T08:25:36.989693Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 60000 entries, 0 to 59999\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      60000 non-null  int64 \n 1   text    60000 non-null  object\n 2   label   60000 non-null  int64 \ndtypes: int64(2), object(1)\nmemory usage: 1.4+ MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"we don't have any nulls","metadata":{}},{"cell_type":"code","source":"df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T08:26:44.166133Z","iopub.execute_input":"2024-04-20T08:26:44.167998Z","iopub.status.idle":"2024-04-20T08:26:44.225614Z","shell.execute_reply.started":"2024-04-20T08:26:44.167939Z","shell.execute_reply":"2024-04-20T08:26:44.224232Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"we don't have any duplicates in our data but let's cheak if we have duplicates in text column","metadata":{}},{"cell_type":"code","source":"df['text'].duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:47:23.791847Z","iopub.execute_input":"2024-04-20T19:47:23.792202Z","iopub.status.idle":"2024-04-20T19:47:23.818209Z","shell.execute_reply.started":"2024-04-20T19:47:23.792174Z","shell.execute_reply":"2024-04-20T19:47:23.817455Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"355"},"metadata":{}}]},{"cell_type":"markdown","source":"we have some of them, let's drop it ","metadata":{}},{"cell_type":"code","source":"df.drop_duplicates('text', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:47:27.860576Z","iopub.execute_input":"2024-04-20T19:47:27.861163Z","iopub.status.idle":"2024-04-20T19:47:27.878235Z","shell.execute_reply.started":"2024-04-20T19:47:27.861132Z","shell.execute_reply":"2024-04-20T19:47:27.877248Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"let's cheak again","metadata":{}},{"cell_type":"code","source":"df['text'].duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T08:32:21.216837Z","iopub.execute_input":"2024-04-20T08:32:21.217349Z","iopub.status.idle":"2024-04-20T08:32:21.236280Z","shell.execute_reply.started":"2024-04-20T08:32:21.217296Z","shell.execute_reply":"2024-04-20T08:32:21.234820Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=df,x='label')","metadata":{"execution":{"iopub.status.busy":"2024-04-20T08:34:03.547840Z","iopub.execute_input":"2024-04-20T08:34:03.548892Z","iopub.status.idle":"2024-04-20T08:34:03.796458Z","shell.execute_reply.started":"2024-04-20T08:34:03.548842Z","shell.execute_reply":"2024-04-20T08:34:03.795061Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<Axes: xlabel='label', ylabel='count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"We can remove rows where the label equals 2 from the dataset to convert this into a binary classification problem.","metadata":{}},{"cell_type":"code","source":"indices_to_drop = df[df['label'] == 2].index\ndf = df.drop(indices_to_drop)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:47:37.568301Z","iopub.execute_input":"2024-04-20T19:47:37.568662Z","iopub.status.idle":"2024-04-20T19:47:37.583110Z","shell.execute_reply.started":"2024-04-20T19:47:37.568635Z","shell.execute_reply":"2024-04-20T19:47:37.582294Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-04-20T08:44:17.714992Z","iopub.execute_input":"2024-04-20T08:44:17.715583Z","iopub.status.idle":"2024-04-20T08:44:17.726996Z","shell.execute_reply.started":"2024-04-20T08:44:17.715537Z","shell.execute_reply":"2024-04-20T08:44:17.725550Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array([0, 1])"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Cleaning and preprocessing**\n","metadata":{}},{"cell_type":"markdown","source":"Create a function to clean training data by removing HTML tags, digits, and single letters, converting all text to lowercase, replacing multiple whitespaces with a single one, removing stopwords and punctuation, and applying stemming. This will help reduce noise and improve data quality for model training.","metadata":{}},{"cell_type":"code","source":"# required package for tokenization.\nnltk.download('punkt') \nnltk.download('stopwords')\n\n# for stemming algorithm\nstemmer = SnowballStemmer(\"english\")\nstop_words = set(stopwords.words(\"english\"))\n\n\n# Make Function to clean text\ndef clean_text(text, for_embedding=False):\n    \"\"\" steps:\n        - remove any html tags (< /br> often found)\n        - Keep only ASCII + European Chars and whitespace, no digits\n        - remove single letter chars\n        - convert all whitespaces (tabs etc.) to single wspace\n        if not for embedding (but e.g. tdf-idf):\n        - all lowercase\n        - remove stopwords, punctuation and stemm\n        - return the clean text\n    \"\"\"\n    re_wspace = re.compile(r\"\\s+\", re.IGNORECASE)\n    re_tags = re.compile(r\"<[^>]+>\")\n    re_ASII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n    re_single_char = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n    if for_embedding:\n        # Keep punctuation\n        re_ASII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n        re_single_char = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n\n    text = re.sub(re_tags, \" \", text)\n    text = re.sub(re_ASII, \" \", text)\n    text = re.sub(re_single_char, \" \", text)\n    text = re.sub(re_wspace, \" \", text)\n\n    word_tokens = word_tokenize(text)\n    words_tokens_lower = [word.lower() for word in word_tokens]\n\n    if for_embedding:\n        # no stemming, lowering and punctuation / stop words removal\n        words_filtered = word_tokens\n    else:\n        words_filtered = [\n            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n        ]\n\n    text_clean = \" \".join(words_filtered)\n    return text_clean","metadata":{"execution":{"iopub.status.busy":"2024-04-20T09:30:28.341853Z","iopub.execute_input":"2024-04-20T09:30:28.342456Z","iopub.status.idle":"2024-04-20T09:30:28.502700Z","shell.execute_reply.started":"2024-04-20T09:30:28.342423Z","shell.execute_reply":"2024-04-20T09:30:28.501815Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# Clean texts training data \ndf[\"text_clean\"] = df.loc[df[\"text\"].str.len() > 0, \"text\"] # get all text data the length it greater than 0 in training data\n# call clean_text of method to apply it on text_clean feature in traing data\ndf[\"text_clean\"] = df[\"text_clean\"].map(\n    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x  # check if text is instance of string\n) ","metadata":{"execution":{"iopub.status.busy":"2024-04-20T09:30:34.746618Z","iopub.execute_input":"2024-04-20T09:30:34.747254Z","iopub.status.idle":"2024-04-20T09:31:06.234719Z","shell.execute_reply.started":"2024-04-20T09:30:34.747223Z","shell.execute_reply":"2024-04-20T09:31:06.233901Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"CPU times: user 31.5 s, sys: 18 ms, total: 31.5 s\nWall time: 31.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-04-20T08:53:11.722980Z","iopub.execute_input":"2024-04-20T08:53:11.723771Z","iopub.status.idle":"2024-04-20T08:53:11.742775Z","shell.execute_reply.started":"2024-04-20T08:53:11.723732Z","shell.execute_reply":"2024-04-20T08:53:11.741160Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"           id                                               text  label  \\\n0      265723  A group of friends began to volunteer at a hom...      0   \n1      284269  British Prime Minister @Theresa_May on Nerve A...      0   \n2      207715  In 1961, Goodyear released a kit that allows P...      0   \n3      551106  Happy Birthday, Bob Barker! The Price Is Right...      0   \n4        8584  Obama to Nation: 聙\"Innocent Cops and Unarmed Y...      0   \n...       ...                                                ...    ...   \n59995   70046  Finish Sniper Simo H盲yh盲 during the invasion o...      0   \n59996  189377  Nigerian Prince Scam took $110K from Kansas ma...      1   \n59997   93486  Is It Safe To Smoke Marijuana During Pregnancy...      0   \n59998  140950  Julius Caesar upon realizing that everyone in ...      0   \n59999   34509  Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...      1   \n\n                                              text_clean  \n0      group friend began volunt homeless shelter nei...  \n1      british prime minist theresa may nerv attack f...  \n2      goodyear releas kit allow ps brought heel http...  \n3      happi birthday bob barker price right host lik...  \n4      obama nation innoc cop unarm young black men d...  \n...                                                  ...  \n59995     finish sniper simo yh invas finland ussr color  \n59996  nigerian princ scam took kansa man year later ...  \n59997       safe smoke marijuana pregnanc surpris answer  \n59998  julius caesar upon realiz everyon room knife e...  \n59999  jeff bridg releas leep tape new album design h...  \n\n[59413 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>text_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>265723</td>\n      <td>A group of friends began to volunteer at a hom...</td>\n      <td>0</td>\n      <td>group friend began volunt homeless shelter nei...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>284269</td>\n      <td>British Prime Minister @Theresa_May on Nerve A...</td>\n      <td>0</td>\n      <td>british prime minist theresa may nerv attack f...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>207715</td>\n      <td>In 1961, Goodyear released a kit that allows P...</td>\n      <td>0</td>\n      <td>goodyear releas kit allow ps brought heel http...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>551106</td>\n      <td>Happy Birthday, Bob Barker! The Price Is Right...</td>\n      <td>0</td>\n      <td>happi birthday bob barker price right host lik...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8584</td>\n      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Y...</td>\n      <td>0</td>\n      <td>obama nation innoc cop unarm young black men d...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>70046</td>\n      <td>Finish Sniper Simo H盲yh盲 during the invasion o...</td>\n      <td>0</td>\n      <td>finish sniper simo yh invas finland ussr color</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>189377</td>\n      <td>Nigerian Prince Scam took $110K from Kansas ma...</td>\n      <td>1</td>\n      <td>nigerian princ scam took kansa man year later ...</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>93486</td>\n      <td>Is It Safe To Smoke Marijuana During Pregnancy...</td>\n      <td>0</td>\n      <td>safe smoke marijuana pregnanc surpris answer</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>140950</td>\n      <td>Julius Caesar upon realizing that everyone in ...</td>\n      <td>0</td>\n      <td>julius caesar upon realiz everyon room knife e...</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>34509</td>\n      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New...</td>\n      <td>1</td>\n      <td>jeff bridg releas leep tape new album design h...</td>\n    </tr>\n  </tbody>\n</table>\n<p>59413 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n# Clean texts testing data\ndf2[\"text_clean\"] = df2.loc[df2[\"text\"].str.len() > 0, \"text\"] # get all text data the length it greater than 0 in testing data\n# call clean_text of method to apply it on text_clean feature in testing data\ndf2[\"text_clean\"] = df2[\"text_clean\"].map(\n    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x  # check if text is instance of string\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T09:31:14.337873Z","iopub.execute_input":"2024-04-20T09:31:14.338203Z","iopub.status.idle":"2024-04-20T09:31:30.284568Z","shell.execute_reply.started":"2024-04-20T09:31:14.338179Z","shell.execute_reply":"2024-04-20T09:31:30.283663Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"CPU times: user 15.9 s, sys: 9.13 ms, total: 15.9 s\nWall time: 15.9 s\n","output_type":"stream"}]},{"cell_type":"code","source":"df2","metadata":{"execution":{"iopub.status.busy":"2024-04-20T08:56:29.257423Z","iopub.execute_input":"2024-04-20T08:56:29.258036Z","iopub.status.idle":"2024-04-20T08:56:29.276079Z","shell.execute_reply.started":"2024-04-20T08:56:29.257993Z","shell.execute_reply":"2024-04-20T08:56:29.274404Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"          id                                               text  \\\n0          0                                         stargazer    \n1          1                                               yeah   \n2          2  PD: Phoenix car thief gets instructions from Y...   \n3          3  As Trump Accuses Iran, He Has One Problem: His...   \n4          4                       \"Believers\" - Hezbollah 2011   \n...      ...                                                ...   \n59146  59146                  Bicycle taxi drivers of New Delhi   \n59147  59147  Trump blows up GOP's formula for winning House...   \n59148  59148  Napoleon returns from his exile on the island ...   \n59149  59149   Deep down he always wanted to be a ballet dancer   \n59150  59150  Toddler miraculously survives 6-story fall lan...   \n\n                                            text_clean  \n0                                              stargaz  \n1                                                 yeah  \n2       pd phoenix car thief get instruct youtub video  \n3                 trump accus iran one problem credibl  \n4                                     believ hezbollah  \n...                                                ...  \n59146                     bicycl taxi driver new delhi  \n59147             trump blow gop formula win hous race  \n59148  napoleon return exil island elba march colouris  \n59149                    deep alway want ballet dancer  \n59150       toddler miracul surviv stori fall land car  \n\n[59151 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>text_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>stargazer</td>\n      <td>stargaz</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>yeah</td>\n      <td>yeah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>PD: Phoenix car thief gets instructions from Y...</td>\n      <td>pd phoenix car thief get instruct youtub video</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>As Trump Accuses Iran, He Has One Problem: His...</td>\n      <td>trump accus iran one problem credibl</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>\"Believers\" - Hezbollah 2011</td>\n      <td>believ hezbollah</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59146</th>\n      <td>59146</td>\n      <td>Bicycle taxi drivers of New Delhi</td>\n      <td>bicycl taxi driver new delhi</td>\n    </tr>\n    <tr>\n      <th>59147</th>\n      <td>59147</td>\n      <td>Trump blows up GOP's formula for winning House...</td>\n      <td>trump blow gop formula win hous race</td>\n    </tr>\n    <tr>\n      <th>59148</th>\n      <td>59148</td>\n      <td>Napoleon returns from his exile on the island ...</td>\n      <td>napoleon return exil island elba march colouris</td>\n    </tr>\n    <tr>\n      <th>59149</th>\n      <td>59149</td>\n      <td>Deep down he always wanted to be a ballet dancer</td>\n      <td>deep alway want ballet dancer</td>\n    </tr>\n    <tr>\n      <th>59150</th>\n      <td>59150</td>\n      <td>Toddler miraculously survives 6-story fall lan...</td>\n      <td>toddler miracul surviv stori fall land car</td>\n    </tr>\n  </tbody>\n</table>\n<p>59151 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Modeling**","metadata":{}},{"cell_type":"code","source":"#split the data into train and test using train_test_split\nX = df['text']\n# contain the label\ny = df['label']\nX_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,test_size=0.2)\n\nX_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train, random_state = 7)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T09:31:35.133710Z","iopub.execute_input":"2024-04-20T09:31:35.134535Z","iopub.status.idle":"2024-04-20T09:31:35.190946Z","shell.execute_reply.started":"2024-04-20T09:31:35.134505Z","shell.execute_reply":"2024-04-20T09:31:35.190124Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Trail 1**","metadata":{}},{"cell_type":"markdown","source":"in this trail i will try CountVectorizer with XGBoost classifire","metadata":{}},{"cell_type":"code","source":"is_train = np.isin(X_train.index, X_train2.index)\n\nsplit_index = np.where(is_train, -1, 0)\n\n# Create the PredefinedSplit using the split index\nsplit = PredefinedSplit(test_fold=split_index)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:45:48.837257Z","iopub.execute_input":"2024-04-20T14:45:48.838134Z","iopub.status.idle":"2024-04-20T14:45:48.846381Z","shell.execute_reply.started":"2024-04-20T14:45:48.838100Z","shell.execute_reply":"2024-04-20T14:45:48.845584Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# creat pipeline with CountVectorizer\npipeline = Pipeline([\n    ('cvec', CountVectorizer()),    \n    ('xgboost', XGBClassifier(objective='binary:logistic'))\n])\n\n# Define the parameter grid for GridSearchCV\nparam_grid = {\n    'xgboost__n_estimators': [300, 500], \n    'xgboost__max_depth': [20, 40, 60]\n}\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=split, verbose=2, scoring='roc_auc', n_jobs=-1)\n\n# Fit the grid search to the training data\ngrid_search.fit(X_train, y_train)\n\n# Output the best score from the grid search and scores on train and test sets\nprint(\"Best score:\", grid_search.best_score_)\nprint(\"Train score:\", grid_search.score(X_train, y_train))\nprint(\"Test score:\", grid_search.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T09:31:46.907476Z","iopub.execute_input":"2024-04-20T09:31:46.908216Z","iopub.status.idle":"2024-04-20T09:48:44.979057Z","shell.execute_reply.started":"2024-04-20T09:31:46.908176Z","shell.execute_reply":"2024-04-20T09:48:44.978007Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 6 candidates, totalling 6 fits\n[CV] END ...xgboost__max_depth=40, xgboost__n_estimators=300; total time= 7.4min\n[CV] END ...xgboost__max_depth=40, xgboost__n_estimators=500; total time= 9.8min\n[CV] END ...xgboost__max_depth=20, xgboost__n_estimators=300; total time= 3.7min\n[CV] END ...xgboost__max_depth=60, xgboost__n_estimators=300; total time= 8.1min\nBest score: 0.8906109207462446\nTrain score: 0.9988543678705306\nTest score: 0.9013679951749658\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = df2['id']\nsubmission['label'] = grid_search.predict_proba(df2['text'])[:, 1]\n\nsubmission.to_csv('trail1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T09:51:44.039311Z","iopub.execute_input":"2024-04-20T09:51:44.040086Z","iopub.status.idle":"2024-04-20T09:51:47.149896Z","shell.execute_reply.started":"2024-04-20T09:51:44.040048Z","shell.execute_reply":"2024-04-20T09:51:47.149121Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[CV] END ...xgboost__max_depth=20, xgboost__n_estimators=500; total time= 5.3min\n[CV] END ...xgboost__max_depth=60, xgboost__n_estimators=500; total time= 9.7min\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**trail 2**","metadata":{}},{"cell_type":"markdown","source":"i will try TfidfVectorizer and logistic regression to enhance the accuracy","metadata":{}},{"cell_type":"code","source":"# Define the pipeline with TF-IDF Vectorizer and Logistic Regression\npipeline = Pipeline([\n    ('tfidf_vector', TfidfVectorizer()),  # Text vectorization\n    ('lr', LogisticRegression(solver='liblinear'))  # Logistic regression model\n])\n\n# Define parameter grid for GridSearchCV\nparam_grid = {\n    'tfidf_vector__max_df': [0.5, 0.6, 0.75, 0.98, 1.0],  \n    'tfidf_vector__min_df': [2, 3, 5, 10],  \n    'tfidf_vector__ngram_range': [(1, 1), (1, 2), (1, 3)],  \n    'lr__C': [0.01, 1],  \n    'lr__penalty': ['l2'], \n    'lr__solver': ['lbfgs', 'liblinear']  \n}\n\n# Create a GridSearchCV object\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=split, verbose=2, scoring='roc_auc', n_jobs=-1)\n\n# Fit GridSearchCV to the training data\ngrid_search.fit(X_train, y_train)\n\n# Output the best score from GridSearchCV and evaluate performance on training and test sets\nprint(\"Best score:\", grid_search.best_score_)\nprint(\"Train score:\", grid_search.score(X_train, y_train))\nprint(\"Test score:\", grid_search.score(X_test, y_test))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:03:09.017976Z","iopub.execute_input":"2024-04-20T10:03:09.018716Z","iopub.status.idle":"2024-04-20T10:08:41.223815Z","shell.execute_reply.started":"2024-04-20T10:03:09.018663Z","shell.execute_reply":"2024-04-20T10:08:41.222700Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 240 candidates, totalling 240 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"},{"name":"stdout","text":"[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.5s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.5s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   5.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.6s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   8.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.5s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   5.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   8.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   5.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.0s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   4.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   6.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   4.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.0s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   4.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.5s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=  11.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   6.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   5.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   3.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=  11.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   8.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   6.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   6.1s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   3.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   3.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   6.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   6.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.8s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   8.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   5.0s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   9.0s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   5.0s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   8.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   7.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   3.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   5.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   3.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   3.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=  10.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   5.5s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   7.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   3.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   5.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   3.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   7.5s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   3.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   8.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=  11.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   3.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   8.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   5.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   5.0s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.8s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.8s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.8s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.0s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.4s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.5s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   6.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   6.8s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.5s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   6.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   6.8s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   9.1s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   5.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   7.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   3.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   8.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   3.1s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   7.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   8.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=  12.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   3.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   9.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   5.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   4.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   7.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   8.5s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.0s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   8.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.7s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   4.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   4.9s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   5.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   4.8s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.1s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.0s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=0.01, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   3.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   3.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   6.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.5, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   8.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=  12.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.6, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   9.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   5.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   3.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   9.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   5.5s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   7.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   3.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=lbfgs, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   9.1s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   8.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.5, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   4.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.9s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   4.8s\nBest score: 0.8983428017149577\nTrain score: 0.9557472165117035\nTest score: 0.9010402484083457\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = df2['id']\nsubmission['label'] = grid_search.predict_proba(df2['text'])[:, 1]\n\nsubmission.to_csv('trail2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T10:11:39.646724Z","iopub.execute_input":"2024-04-20T10:11:39.647595Z","iopub.status.idle":"2024-04-20T10:11:41.025475Z","shell.execute_reply.started":"2024-04-20T10:11:39.647562Z","shell.execute_reply":"2024-04-20T10:11:41.024723Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.1s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.6, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 1); total time=   2.5s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 2); total time=   5.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 1); total time=   2.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 2); total time=   4.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   7.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   4.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   6.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   7.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.0s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 2); total time=   4.8s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=2, tfidf_vector__ngram_range=(1, 3); total time=   7.7s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 1); total time=   2.3s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 2); total time=   4.1s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.75, tfidf_vector__min_df=5, tfidf_vector__ngram_range=(1, 3); total time=   7.2s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 1); total time=   2.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=0.98, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   7.6s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=3, tfidf_vector__ngram_range=(1, 3); total time=   7.4s\n[CV] END lr__C=1, lr__penalty=l2, lr__solver=liblinear, tfidf_vector__max_df=1.0, tfidf_vector__min_df=10, tfidf_vector__ngram_range=(1, 3); total time=   5.5s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"it's give me the best accuracy on the leaderboard","metadata":{}},{"cell_type":"markdown","source":"**trail 3** ","metadata":{}},{"cell_type":"markdown","source":"in this trail i'll switch to using CountVectorizer for feature extraction to evaluate the differences between utilizing 'word' and 'char' analyzers","metadata":{}},{"cell_type":"code","source":"\ntf_vector = TfidfVectorizer(stop_words=\"english\", analyzer='word', ngram_range=(1, 2), max_df=0.5, min_df=10)\nX_transformed = tf_vector.fit_transform(X_train)\nprint('This vectorizer will produce', X_transformed.shape[1], 'columns.')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T15:14:05.643282Z","iopub.execute_input":"2024-04-20T15:14:05.643709Z","iopub.status.idle":"2024-04-20T15:14:07.611620Z","shell.execute_reply.started":"2024-04-20T15:14:05.643677Z","shell.execute_reply":"2024-04-20T15:14:07.610665Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"This vectorizer will produce 7778 columns.\n","output_type":"stream"}]},{"cell_type":"code","source":"tf_vector = TfidfVectorizer(analyzer='char', ngram_range=(1, 2))\nX_transformed = tf_vector.fit_transform(X_train)\nprint('This vectorizer will produce', X_transformed.shape[1], 'columns.')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T15:15:38.456103Z","iopub.execute_input":"2024-04-20T15:15:38.456468Z","iopub.status.idle":"2024-04-20T15:15:42.540498Z","shell.execute_reply.started":"2024-04-20T15:15:38.456444Z","shell.execute_reply":"2024-04-20T15:15:42.539588Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"This vectorizer will produce 735 columns.\n","output_type":"stream"}]},{"cell_type":"code","source":"# using the tfidf as vectorizer with char analyzer\npipe = Pipeline([('tfidf_vector', TfidfVectorizer( ngram_range=(1, 2))),    \n                 #using logistic regression as model\n                 ('lr', LogisticRegression(solver='liblinear'))])\n\n# Tune by  GridSearchCV\npipe_params = {'tfidf_vector__analyzer':['char','word'],\n    'lr__C': [0.01, 1],}\n\n\ngs_trial = GridSearchCV(pipe, param_grid=pipe_params, cv=split,verbose=2,scoring='roc_auc',n_jobs=-1)\ngs_trial.fit(X_train, y_train);\nprint(\"Best score:\", gs_trial.best_score_)\nprint(\"Train score\", gs_trial.score(X_train, y_train))\nprint(\"Test score\", gs_trial.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T15:16:15.366945Z","iopub.execute_input":"2024-04-20T15:16:15.367316Z","iopub.status.idle":"2024-04-20T15:16:30.370623Z","shell.execute_reply.started":"2024-04-20T15:16:15.367287Z","shell.execute_reply":"2024-04-20T15:16:30.369601Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 4 candidates, totalling 4 fits\nBest score: 0.8708529752835528\nTrain score 0.9702667551580434\nTest score 0.8733725410089437\n[CV] END ............lr__C=0.01, tfidf_vector__analyzer=word; total time=   5.0s\n[CV] END ...............lr__C=1, tfidf_vector__analyzer=word; total time=   5.2s\n[CV] END ............lr__C=0.01, tfidf_vector__analyzer=char; total time=   6.8s\n[CV] END ...............lr__C=1, tfidf_vector__analyzer=char; total time=   7.4s\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = df2['id']\nsubmission['label'] = grid_search.predict_proba(df2['text'])[:, 1]\n\nsubmission.to_csv('trail3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T11:44:50.004095Z","iopub.execute_input":"2024-04-20T11:44:50.005149Z","iopub.status.idle":"2024-04-20T11:44:51.444054Z","shell.execute_reply.started":"2024-04-20T11:44:50.005108Z","shell.execute_reply":"2024-04-20T11:44:51.443163Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[CV] END ............lr__C=0.01, tfidf_vector__analyzer=word; total time=   6.4s\n[CV] END ...............lr__C=1, tfidf_vector__analyzer=word; total time=   7.0s\n[CV] END ............lr__C=0.01, tfidf_vector__analyzer=char; total time=   9.3s\n[CV] END ...............lr__C=1, tfidf_vector__analyzer=char; total time=  10.6s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**trail 4**","metadata":{}},{"cell_type":"markdown","source":"In this trial, I will evaluate the impact of stemming on text classification by experimenting with two different classifiers: RandomForest and SVM","metadata":{}},{"cell_type":"code","source":"# Initialize stemmer and stopwords\nstemmer = SnowballStemmer('english')\nstop_words = set(stopwords.words('english'))\n\ndef clean_text(text):\n    text = text.lower().split('\\t')[0]\n    text = re.sub(r'\\W', ' ', text)\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r\"[^a-z ]\", ' ', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    # Tokenize, stem, and remove stopwords\n    tokens = word_tokenize(text)\n    stemmed = [stemmer.stem(word) for word in tokens if word not in stop_words]\n    return ' '.join(stemmed)\n\n# Load dataset and preprocess\ndf = pd.read_csv('/kaggle/input/cisc-873-dm-w24-a3/xy_train.csv')\ndf = df[df['label'] != 2]\ndf.drop_duplicates('text', inplace=True)\ndf['text'] = df['text'].apply(clean_text)\n\n# Split dataset into features and target\nX = df['text']\ny = df['label']\n\n# Split data into train-test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7, stratify=y)\n\n# Further split training data into training and validation sets\nX_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=7, stratify=y_train)\n\n# Create a PredefinedSplit based on the indices of X_train2\nis_train = np.isin(X_train.index, X_train2.index)\n\nsplit_index = np.where(is_train, -1, 0)\n\n# Create the PredefinedSplit using the split index\nsplit = PredefinedSplit(test_fold=split_index)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:48:08.808642Z","iopub.execute_input":"2024-04-20T19:48:08.808995Z","iopub.status.idle":"2024-04-20T19:48:39.847835Z","shell.execute_reply.started":"2024-04-20T19:48:08.808967Z","shell.execute_reply":"2024-04-20T19:48:39.847058Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\n\n# Define the pipeline with a placeholder for the classifier\npipe = Pipeline([\n    ('tfidf_vector', TfidfVectorizer()),    \n    ('classifier', None)  # Placeholder for the classifier\n])\n\n# Define parameter grid with options for RandomForest and SVC\nparam_grid = [\n    {\n        'classifier': [RandomForestClassifier()],\n        'classifier__n_estimators': [100, 300, 500],\n        'classifier__max_depth': [None, 10, 20, 30],\n        'classifier__min_samples_split': [2, 5, 10]\n    },\n    {\n        'classifier': [SVC(probability=True)],\n        'classifier__C': [0.1, 1, 10],\n        'classifier__kernel': ['linear', 'rbf'],\n        'classifier__gamma': ['scale', 'auto']\n    }\n]\n\n# Create a GridSearchCV object with the pipeline and parameter grid\ngs_trial = GridSearchCV(pipe, param_grid=param_grid, cv=split, verbose=2, scoring='roc_auc', n_jobs=-1)\n\n# Fit GridSearchCV to the training data\ngs_trial.fit(X_train, y_train)\n\n# Output the best score from GridSearchCV and evaluate performance on training and test sets\nprint(\"Best score:\", gs_trial.best_score_)\nprint(\"Train score:\", gs_trial.score(X_train, y_train))\nprint(\"Test score:\", gs_trial.score(X_test, y_test))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T19:48:47.360518Z","iopub.execute_input":"2024-04-20T19:48:47.361189Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 48 candidates, totalling 48 fits\n[CV] END classifier=RandomForestClassifier(), classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=100; total time= 1.8min\n[CV] END classifier=RandomForestClassifier(), classifier__max_depth=None, classifier__min_samples_split=5, classifier__n_estimators=300; total time= 5.3min\n[CV] END classifier=RandomForestClassifier(), classifier__max_depth=None, classifier__min_samples_split=10, classifier__n_estimators=300; total time= 4.5min\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = df2['id']\nsubmission['label'] = gs_trial.predict_proba(df2['text'])[:, 1]\n\nsubmission.to_csv('trail6.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T18:05:32.165430Z","iopub.execute_input":"2024-04-20T18:05:32.165786Z","iopub.status.idle":"2024-04-20T18:05:32.251700Z","shell.execute_reply.started":"2024-04-20T18:05:32.165761Z","shell.execute_reply":"2024-04-20T18:05:32.250336Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m submission \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      2\u001b[0m submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m submission[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgs_trial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m(df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m submission\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrail6.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_available_if.py:32\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m     26\u001b[0m attr_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(owner\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[1;32m     34\u001b[0m     out \u001b[38;5;241m=\u001b[39m MethodType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, obj)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:362\u001b[0m, in \u001b[0;36m_estimator_has.<locals>.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    359\u001b[0m _check_refit(\u001b[38;5;28mself\u001b[39m, attr)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_estimator_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# raise an AttributeError if `attr` does not exist\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# raise an AttributeError if `attr` does not exist\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_available_if.py:32\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m     26\u001b[0m attr_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(owner\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[1;32m     34\u001b[0m     out \u001b[38;5;241m=\u001b[39m MethodType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, obj)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py:46\u001b[0m, in \u001b[0;36m_final_estimator_has.<locals>.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# raise original `AttributeError` if `attr` does not exist\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/_available_if.py:32\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m     26\u001b[0m attr_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(owner\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[1;32m     34\u001b[0m     out \u001b[38;5;241m=\u001b[39m MethodType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, obj)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:829\u001b[0m, in \u001b[0;36mBaseSVC._check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobability:\n\u001b[0;32m--> 829\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    830\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba is not available when  probability=False\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m         )\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_svc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnu_svc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_proba only implemented for SVC and NuSVC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"],"ename":"AttributeError","evalue":"predict_proba is not available when  probability=False","output_type":"error"}]},{"cell_type":"markdown","source":"**trail 5**","metadata":{}},{"cell_type":"markdown","source":"In this trial, I will evaluate the impact of lematizing on text classification","metadata":{}},{"cell_type":"code","source":"# will lematizer the words\nlemmatizer = WordNetLemmatizer() \nstop_words = set(stopwords.words(\"english\")) \ndef clean_text(text):\n     # lower case the text\n    text=text.lower()\n    line=text.split('\\t')[0]\n\n    # concat the list in one line\n    text=''.join(line)\n\n    # remove the special charachters\n    text = re.sub(r'\\W', ' ', str(text))\n\n    # remove all single characters\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n    # remove the white spaces\n    text= re.sub(r'\\d+', '',text)\n    # remove non english charachters\n    text= re.sub(r\"[^A-Za-zÀ-ž ]\", ' ',text)\n    # remove the white spaces\n    text= re.sub(r'\\s+', ' ',text)\n    text = word_tokenize(text)\n    text = [stemmer.stem(word) for word in text if word not in stop_words]\n\n    text=' '.join(text)\n       \n    return text\n\n     \n","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:20:15.604349Z","iopub.execute_input":"2024-04-20T14:20:15.605364Z","iopub.status.idle":"2024-04-20T14:20:15.621606Z","shell.execute_reply.started":"2024-04-20T14:20:15.605330Z","shell.execute_reply":"2024-04-20T14:20:15.620819Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/cisc-873-dm-w24-a3/xy_train.csv')\ndf=df[df['label']!=2]\ndf.drop_duplicates('text', inplace=True)\ndf['text']=df['text'].apply(clean_text)\nX = df['text']\n# contain the label\ny = df['label']\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=7,stratify=y,test_size=0.2)\n\nX_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train, random_state = 7)\nsplit_index = [-1 if x in X_train2.index else 0 for x in X_train.index]\nsplit= PredefinedSplit(test_fold = split_index)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:20:53.064886Z","iopub.execute_input":"2024-04-20T14:20:53.065573Z","iopub.status.idle":"2024-04-20T14:21:24.513617Z","shell.execute_reply.started":"2024-04-20T14:20:53.065544Z","shell.execute_reply":"2024-04-20T14:21:24.512784Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# using the tfidf as vectorizer\npipe = Pipeline([('tfidf_vector', TfidfVectorizer()),    \n                 #using random forest as model\n                 ('rf', RandomForestClassifier())])\n\n# Tune by  GridSearchCV\npipe_params = {'rf__n_estimators': [600],\n              'rf__max_depth':[30,40,50]}\n\ngs_trial = GridSearchCV(pipe, param_grid=pipe_params, cv=split,verbose=2,scoring='roc_auc',n_jobs=-1)\ngs_trial.fit(X_train, y_train);\nprint(\"Best score:\", gs_trial.best_score_)\nprint(\"Train score\", gs_trial.score(X_train, y_train))\nprint(\"Test score\", gs_trial.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:21:41.845109Z","iopub.execute_input":"2024-04-20T14:21:41.845510Z","iopub.status.idle":"2024-04-20T14:24:03.103153Z","shell.execute_reply.started":"2024-04-20T14:21:41.845483Z","shell.execute_reply":"2024-04-20T14:24:03.101991Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fitting 1 folds for each of 3 candidates, totalling 3 fits\nBest score: 0.833046072037342\nTrain score 0.9520189435373383\nTest score 0.8373534306146363\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['id'] = df2['id']\nsubmission['label'] = gs_trial.predict_proba(df2['text'])[:, 1]\n\nsubmission.to_csv('trail5.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T14:24:38.567020Z","iopub.execute_input":"2024-04-20T14:24:38.567674Z","iopub.status.idle":"2024-04-20T14:24:46.429812Z","shell.execute_reply.started":"2024-04-20T14:24:38.567642Z","shell.execute_reply":"2024-04-20T14:24:46.428967Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[CV] END .............rf__max_depth=30, rf__n_estimators=600; total time=  36.4s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Problem formulation:\n\n### Define the problem:\n\n- The dataset provided to us is the title of the news and a label of this title whether it is fake or not. Since spreading fake news is a big problem in our life, we aim to contribute to the solving of this problem through data mining and machine learning. Therefore, we can formulate a binary classification task that we have to address.\n\n### what is the input?\n\n- The input is a text feature. It encompasses various forms of words.\n\n### what is the output ?\n- A probability between 0-1 is used to determine if the news is fake or not. We can set a threshold to penalize fake news; for instance, if the model predicts that the title tends to be fake with a probability of 0.7 (though we often use 0.5), why not consider changing the threshold?\n\n### what is the data mining function required ?\n\n - The binary classification function, which is the mining function employed in this challenge, was achieved by taking the following actions:\n- **data gathering**\n- **Data cleaning and preprocessing**\n\n- **modeling** Identify the problem as a classification problem, ascertain the evaluation measure (in our instance, ACUC), and then experiment with various models using various hyperparameters and features using grid search, random search, and bayesian search techniques.\n- **Model evaluation involves comparing the best scoring measure to determine which model is the best.\n\n\n### what are the challanges?\n\n- There are several word forms in the data.\n- There are outlier values in the datasets.\n- Determine whether a particular Reddit post is fake news or not based only on its title.\n\n### what is the impact ?\n- The creation of such a high-performance model will help to significantly reduce the spread of rumors and fake news, and thereby improve the user experience on various platforms.\n\n### what is the ideal solution ?\n- The ideal solution would be to have a large, clean dataset and high-performance computing resources to develop a model capable of easily classifying fake news. However, the approach I was able to implement involved using logistic regression with TF-IDF. I also explored the state-of-the-art in deep learning, specifically BERT via transfer learning, but it did not yield satisfactory results.\n\n\n### what is th experimental protocol used ?What preprocessing steps are used?\nTo compare many treatments using different models and attributes to one outcome, the AUC score, we used a pre-experimental approach. Here's how this procedure was carried out:\n\nPrepare and handle the experiment's data:\n- Eliminate duplicates.\n- Respond to texts with a '2' label, whose veracity was questioned.\n- Make use of diverse embeddings produced by different analyzers and algorithms.\n- Get rid of stop words.\n- Take out the new lines and tabs.\n- Special characters should be removed.\n- Take out any characters that are not in English.\n- Texts with more words than a certain amount are excluded.\n- To choose the best model, create a variety of models and keep an eye on the AUC score.","metadata":{}},{"cell_type":"markdown","source":"**Questions**\n\n### Q1)What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n- Despite the fact that they are both feature extraction techniques used in natural language processing, the characteristics could be different. N-gram could be defined as a set of co-occurring words within a given window, and the window size could be 1 unigram,2 bigram ,…. so we can say the size of the windows depends and determined by n. Unigrams (n-gram size = 1), Bigrams, terms compounded by two words (n-gram size = 2), Trigrams, terms compounded by up to three words (n-gram size = 3), char n-gram, counting sequences of characters, word n-gram, counting sequences of words, but they produce different embeddings of course, most of the time word n-gram produce very sparse matrix, and as there is a huge number of words in every language, and in data mining we only work on sample not the whole population, we might see OOV (Out Of Vocabulary) issue, that is probably we may see new words which we didn’t see in the training, so char n-gram may handle this issue, as the probability of seeing new character sequence is much less than the probability of seeing new word sequence\n\n### Q2) What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n- Stop word removal involves removing words that frequently occur across documents and do not significantly alter the context of sentences. These words, such as \"the,\" \"a,\" \"have,\" and \"always\" in English, are primarily used to maintain grammatical structure in text. In natural language processing (NLP), the focus is more on the action or the type of word rather than its placement or timing, helping to maintain context integrity without creating biased embeddings.\n\n- Stemming reduces words to their stem form by removing suffixes and prefixes, which helps in text normalization. For instance, \"played,\" \"play,\" \"player,\" and \"plays\" all derive from the same root \"play,\" and stemming them consolidates their representation, enhancing the consistency in embeddings.\n\n- Both techniques are language-dependent. For example, stop words in Arabic differ significantly from those in English. Additionally, languages that use the Latin alphabet may still have different stopwords or stem forms, underscoring the need for language-specific processing tools. Similarly, a stemming algorithm designed for English cannot be used for Spanish as it would not correctly interpret the root forms of Spanish words\n### Q3) Is tokenization techniques language dependent? Why?\n- Tokenization involves splitting a piece of text into individual words using specific delimiters like spaces, commas, or tabs. This process results in the formation of word-level tokens, which are dependent on the language structure. For instance, while English words are typically separated by spaces, Chinese text is written without spaces between successive characters and words, posing a unique challenge for tokenization.\n### Q4 )What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n- CountVectorizer: This method calculates the frequency of each word to create a representation based on embeddings. It's a straightforward approach to understanding text data by evaluating how often words appear.\n- TFIDF (Term Frequency-Inverse Document Frequency): This technique transforms a collection of raw documents into a matrix of TF-IDF features. It focuses on the frequency of words and their relative importance across documents. By emphasizing significant words and ignoring less relevant ones, TFIDF reduces the dimensionality of the model, simplifying the model building process.\n- Regarding the use of all possible n-grams, it's impractical because the size of the feature space would grow exponentially. Therefore, it's common to select an appropriate n-gram size as a hyperparameter and optimize it based on the specific needs of the task. For instance, in sentiment analysis, using bigrams or trigrams can significantly enhance classification accuracy by capturing more complex expressions that single words alone might miss.\n","metadata":{}}]}